{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb941f39-ba19-4498-988b-db2a3b7f97c0",
   "metadata": {},
   "source": [
    "# Stereo Robot Navigation\n",
    "## Federico Calzoni, Lorenza Guerriero, Arka Patra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceafffe-5cb8-485c-9948-5e112aa4ac18",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Sensing of 3D information related to the obstacles in front of the vehicle should rely on the stereo vision principle. The ability of an autonomous vehicle to sense obstacles and navigate its surroundings is a key role in the development of modern robots. For this reason we present a project in which a robot, through the principle of a stereo camera, is able to perceive the obstacles in front of his view. Such technology could be implemented by a navigation system for autonomous vehicles to automatically avoid obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b867cee",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The objective of the project aims to use synchronized video sequences captured by a stereo camera mounted on a moving vehicle to detect spatial information of the frontal environment. The primary task is to develop an area-based stereo matching algorithm capable of generating a dense disparity map from the syncronized stereo camera inputs (robotL.avi for the left view, robotR.avi for the right view). Then by using geometrical calculations an extimated distance from the robot to the central area of the view need to be calculated. Additionally, an alarm is triggered whenever the distance falls below 0.8 meters.\n",
    "Finally, given that in the center of the sciene there is a chessboard. The dimension of it (height and width) must be computed. This will be useful in order to compare it to the real values and give an extimation of the accuracy of the method used.  \n",
    "\n",
    "\n",
    "## Functional Specifications\n",
    "\n",
    "The system follows a series of steps to achieve its objectives:\n",
    "\n",
    "1.  <b> Take the dataset</b>\\\n",
    "To estimate all the points, is useful to take the video and the parameters required to estimate distances from stereo images like focal length (f = 567.2 pixel) and baseline (b = 92.226 mm).\n",
    "\n",
    "2.  <b>Disparity Map Computation</b>\\\n",
    "Compute the disparity map in a central area of the reference frame to sense distances in the portion of the environment relevant to the vehicle's trajectory.\n",
    "\n",
    "3.  <b>Main Disparity Estimation</b>\\\n",
    "Estimate the main disparity for the frontal portion of the environment based on the disparity map of the central area of the reference frame.\n",
    "\n",
    "4.  <b>Distance Calculation</b>\\\n",
    "Determine the distance of the obstacle from the moving vehicle based on the main disparities estimated from each pair of frames.\n",
    "\n",
    "5.  <b>Output Generation and Alarm Trigger</b>\\\n",
    "Generate output conveying distance information to the user and trigger an alarm when the distance falls below a predefined threshold (0.8 m).\n",
    "\n",
    "6.  <b>Estimate the size of the board</b>\\\n",
    "Use the found board corners to estimate the size of the board. Then compare it with the real size of the known one (125mm x 178mm)\n",
    "\n",
    "## Improvements\n",
    "\n",
    "We implement the following improvements:\n",
    "\n",
    "1.  <b>Disparity Map Computation with the offset</b>\\\n",
    "Modify the matching algorithm so to deploy a smaller disparity range with 'o' being a suitable horizontal offset. This offset can be computed as that value allowing the main disparity, dmain, computed at the previous time instant. Accordingly, as the vehicle gets closer to the obstacle, the horizontal offset\n",
    "increases, thus avoiding the main disparity to exceed the disparity range (and conversely, when the vehicle goes away from the obstacle).\n",
    "\n",
    "2.  <b>From one disparity to more</b>\\\n",
    "Instead of just a single main disparity, compute a set of disparities associated with the different areas of the obstacle, so to then estimate the distance from the camera for each part of the obstacle. For example, the image may be divided into a 5 large vertical stripes, assuming the vertical lines parallel to the obstacle to be parallel to the image plane of the stereo sensor, and then estimate for each stripe a main disparity value. Create a planar view of the obstacle computing the angle between the horizontal lines parallel to the obstacle and the image plane of the stereo sensor.\n",
    "\n",
    "3.  <b>Develop a more robust approach to estimate the main disparity</b>\\\n",
    "Ambiguous disparity measurements may be detected and removed by analysing the function representing the dissimilarity (similarity) measure along the disparity range or by computing disparities only at sufficiently textured image points. We prefer to do this using the salient image points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a5194",
   "metadata": {},
   "source": [
    "## Stereo Matching Algorithm\n",
    "\n",
    "The stereo matching algorithm computes the disparity map for each pair of synchronized frames. The algorithm could utilizes the Sum of Absolute Differences funcition (SAD) or Sum of Squared Differences funcition (SSD) for the dissimilarity measure and the Normalized Cross Correlation funcition (NCC) or Zero mean Normalized Cross-Correlation funcition (ZNCC) for the similarity  measure to compare intensities between corresponding points in the left and right images.\n",
    "\n",
    "The matching function can be any function that measures the distortion or the match between the block, in the current frame and the displaced candidate block in the reference frame. The choice of a suitable matching function is very important, for it impacts both the prediction quality and the computational complexity of the algorithm. Follows the four principal techniques for matching function. \n",
    "\n",
    "### Sum of Absolute Differences (SAD)\n",
    "\n",
    "The Sum of Absolute Differences is a measure of the dissimilarity between image blocks. It is calculated by taking the absolute difference between each pixel in the original block and the corresponding pixel in the block being used for comparison.[1]\n",
    "The SAD may be used for a variety of purposes, such as object recognition, the generation of disparity maps for stereo images, and motion estimation for video compression. It compares intensities between corresponding points in rectified images to compute the disparity map.\n",
    "\n",
    "SAD takes every pixel in a block. It takes the sum of absolute difference intensity value of the left image and its candidate disparity. \n",
    "\n",
    "$$SAD (i,j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} |I(i+m, j+n)-T(m,n)|$$ \n",
    "\n",
    "    I(i+m, j+n)  -intensity value of the image;  \n",
    "    T(m, n)      -intensity value of target image.        \n",
    "\n",
    "The SAD represents the L1 norm of the difference between vectors I(i,j) e T.\n",
    "\n",
    "Disparity means the horizontal displacement between the left image and right image. Depth map is also called disparity map.\n",
    "\n",
    "### Sum of Squared Differences (SSD)\n",
    "\n",
    "The term sum of squares refers to a statistical technique used in regression analysis to determine the dispersion of data points. The sum of squares can be used to find the function that best fits by varying the least from the data. In a regression analysis, the goal is to determine how well a data series can be fitted to a function that might help to explain how the data series was generated. [2]\n",
    "\n",
    "The sum of squares measures the deviation of data points away from the mean value.A higher sum of squares indicates higher variability while a lower result indicates low variability from the mean.To calculate the sum of squares, subtract the mean from the data points, square the differences, and add them together.\n",
    "\n",
    "The standard sum of squared differences (SSD) dissimilarity metric is used and the position of the target is found as that giving the lowest dissimilarity score.\n",
    "\n",
    "$$SSD (i,j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} (I (i+m, j+n)-T(m,n))^2$$\n",
    "\n",
    "    I(i+m, j+n) -represent the gray-level values of the image;\n",
    "    T(m, n)     -represent the gray-level values of the template.\n",
    "\n",
    "Both I(i, j) (i.e. the window at position (i, j) of the target image having the same size as T) as well as T can be thought of as MÂ·N-dimensional vectors (flattening). Accordingly, the SSD represents the squared L2 (Euclidean) norm of their difference.\n",
    "\n",
    "The sum of squares is a statistical measure of deviation from the mean. It is also known as variation. It is calculated by adding together the squared differences of each data point. To determine the sum of squares, square the distance between each data point and the line of best fit, then add them together. The line of best fit will minimize this value. A low sum of squares indicates little variation between data sets while a higher one indicates more variation. Variation refers to the difference of each data set from the mean.\n",
    "\n",
    "### Normalized Cross Correlation (NCC)\n",
    "\n",
    "The normalization embodied into the NCC and ZNCC allows for tolerating linear brightness variations. Furthermore, thanks to the subtraction of the local mean, the ZNCC provides better robustness than the NCC since it tolerates uniform brightness variations as well. Since template matching based on the ZNCC or NCC can be very expensive, several non-exhaustive algorithms aimed at speeding-up the matching process have been developed. [3] \n",
    "\n",
    "We compute the NCC value between the left and right processed images to measure the similarity of corresponding pixel locations. The aim of this step is to utilize the underlying geometric cue, since the entire lane markings lie on the road plane and all the road points are mapped into the same global coordinates. For each detected lane marking pixel in the left image, the NCC is computed with the pixel at the same location in the right image: [4]\n",
    "\n",
    "$$NCC(i,j)=\\frac{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I (i+m, j+n)\\cdot T(m,n)}{\\sqrt{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I (i+m, j+n)^2}\\cdot \\sqrt{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} T(m,n)^2}} $$\n",
    "\n",
    "    I(i+m, j+n) -represent the gray-level values of the image;\n",
    "    T(m, n)     -represent the gray-level values of the template.\n",
    "\n",
    "The NCC represents the cosine of the angle between vectors I(i,j) e T.\n",
    "\n",
    "### Zero mean Normalized Cross-Correlation (ZNCC)\n",
    "\n",
    "Zero mean Normalized Cross-Correlation or shorter ZNCC is an integer you can get when you compare two grayscale images. The higher the ZNCC gets, the more are those two images correlated.\n",
    "\n",
    "ZNCC-based template matching that finds the same optimal solution as a full-search process and allows for significant computational savings. The algorithm is a generalization of the Bounded Partial Correlation (BPC) technique, previously devised only for NCC-based template matching.\n",
    "This section describes how to generalize the basic BPC technique based on the Cauchy Schwarz inequality to a template matching process based on the ZNCC function. The novel technique will be referred to as Zero mean Bounded Partial Correlation (ZBPC). [3]\n",
    "\n",
    "Correlation between T and I at pixel position (i,j) can be written as:\n",
    "\n",
    "$$NCC(i,j)=\\frac{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} (I (i+m, j+n) - \\mu (\\tilde I)) \\cdot (T(m,n)-\\mu(T))}{\\sqrt{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} (I (i+m, j+n)-\\mu(\\tilde I))^2}\\cdot \\sqrt{\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} (T(m,n)-\\mu(T))^2}} $$\n",
    "\n",
    "    I(i+m, j+n) -represent the gray-level values of the image;\n",
    "    T(m, n)     -represent the gray-level values of the template;\n",
    "    mu          -the mean intensity value of I and T.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677d9e0",
   "metadata": {},
   "source": [
    "# Distance Estimation\n",
    "\n",
    "Based on the computed disparity map, the system estimates the main disparity for the frontal portion of the environment. Using the provided parameters (focal length and baseline), it calculates the distance (in mm) of obstacles from the moving vehicle.\n",
    "\n",
    "$$ Z(mm)=\\frac {f(pixel)\\cdot b(mm)}{d(pixel)}$$\n",
    "\n",
    "    z     -Depth \n",
    "    f     -Focal length of the camera \n",
    "    b     -Distance between the camera centers\n",
    "    d     -Mean Disparity \n",
    "\n",
    "The depth is directly proportional to the focal length of the camera and the horizontal diaplacement. Different types of camera are used for image capturing, like stereo camera, dynamic vision sensor etc etc. After finding the depth map reconstruction of the 3D information involves several post processing steps like disparity refinement, rectification etc. Application specific architectures based on SAD along with other methods improve the accuracy in depth map. Mostly used methods make use of SAD, with some modifications, which also improve the performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edbe7e",
   "metadata": {},
   "source": [
    "# Dimension of the chessboard \n",
    "\n",
    "First of all we used the OpenCV functions cvFindChessboardCorners and cvDrawChessboardCorners may be deployed to, respectively, find and display the pixel coordinates of the internal corners of the chessboard. After we assuming the chessboard pattern to be parallel to the image plane of the stereo sensor, the real dimensions of the pattern can be obtained from their pixel dimensions (w,h) by the following formulas: \n",
    "\n",
    "$$ W(mm) = \\frac {z(mm)\\cdot w(pixel)}{f(pixel)}$$\n",
    "\n",
    "$$ H(mm) = \\frac {z(mm)\\cdot h(pixel)}{f(pixel)}$$\n",
    "\n",
    "    W       -length of the chessboard\n",
    "    H       -hight of the chessboard\n",
    "    f       -focal length\n",
    "    z       -distance of the obstacle wrt to the moving vehicle\n",
    "\n",
    "We compare the estimated real dimensions to the known ones (125 mm x 178 mm) to verify that accuracy becomes higher as the vehicle gets closer to the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffcdbe",
   "metadata": {},
   "source": [
    "# TODO code and result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4463d33-dfc1-4be3-a4d4-5cd34869850c",
   "metadata": {},
   "source": [
    "# TODO conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4d752-eb49-4751-a9e3-422ad4103b7a",
   "metadata": {},
   "source": [
    "# References\n",
    "    [1] Survey on Stereovision Based Disparity Map Using Sum of Absolute Difference - International Journal of Innovative Science and Research Technology by Parvathy B.H. and Deepambika V.A.\n",
    "    chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://ijisrt.com/wp-content/uploads/2018/01/Survey-on-Stereovision-Based-Disparity-Map-Using-Sum-of-Absolute-Difference.pdf\n",
    "\n",
    "    [2] https://www.investopedia.com/terms/s/sum-of-squares.asp#:~:text=The%20sum%20of%20squares%20is,fit%2C%20then%20add%20them%20together.\n",
    "\n",
    "    [3] ZNCC-based template matching using bounded partial correlation Luigi Di Stefano, Stefano Mattoccia, Federico Tombari. chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/http://labvisione.deis.unibo.it/fede/papers/jprl05.pdf\n",
    "\n",
    "    [4] Robust Road Environment Perception for Navigation in Challenging Scenarios https://www.sciencedirect.com/topics/computer-science/normalized-cross-correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
